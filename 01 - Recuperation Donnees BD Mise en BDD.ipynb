{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214532ac",
   "metadata": {},
   "source": [
    "# <center>Recupération des données </center>\n",
    "# <center>et mise en base de données </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c31a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies pour le Scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658754eb",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5843c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pour la gestion de la BDD\n",
    "import sqlite3   # BDD pour le MOC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910c958",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d847a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "DEB_Pages=1\n",
    "NB_Pages=10#2032\n",
    "liensBD=[]\n",
    "BdD=\"database.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3cba3",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7279d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime # Module pour la gestion temporelle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace44f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database(app):\n",
    "    if not path.exists('./' + BdD):\n",
    "        db.create_all(app)\n",
    "        print('Created Database!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad22f232",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd40c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heure():\n",
    "    x = datetime.now()\n",
    "    return x.strftime(\"%m\"),x.strftime(\"%d\"),x.strftime(\"%H\"),x.strftime(\"%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b894c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Critiques de BD:\n",
    "# Dans cette partie, on récupére (dans une liste), toutes les BD.\n",
    "# Cette liste contient les liens des pages de la BD et données #\n",
    "# la décrivant ; comme l'EAN, un résumé, etc...\n",
    "\n",
    "for idx in range (DEB_Pages, NB_Pages):\n",
    "    if (idx%800!=0 or idx==1341):\n",
    "        dom = lxml.html.fromstring(requests.get('https://www.babelio.com/livres-/bande-dessinee/18?page='+str(idx)).content)\n",
    "        liensBD+=[x for x in dom.xpath('//a/@href') if '/livres/' in x and ('#c') not in x]\n",
    "        Heure=heure()\n",
    "        print(Heure[2],\":\",Heure[3],\"Pages \",str(idx),\" sur \",str(NB_Pages))\n",
    "        time.sleep(9)\n",
    "    else :\n",
    "        time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16629488",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = pd.DataFrame(liensBD)\n",
    "dfl=dfl.rename(columns = {0:'liens'})\n",
    "# On garde une copie des liens récupérer sur le site babelio.com\n",
    "# On ne sauvegarde que la partie\n",
    "dfl.to_csv('CSV_tmp/list.csv',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd3f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl=pd.read_csv('CSV_tmp/list.csv',sep=\"\\t\")\n",
    "dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ceeabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl=dfl.drop('Unnamed: 0',axis=1) # On retire la colonne inutile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2be9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18c8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "liensBD=dfl['liens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81be336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Changer_type(l, dtype=str):\n",
    "    return list(map(dtype, l))\n",
    "\n",
    "\n",
    "def remove_tags(text) :\n",
    "    TAG_RE=re.sub('\\n |\\t|\\n', '', str(text))\n",
    "    return TAG_RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cec8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxsiblocage=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a962d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# POUR LE POC ON SE CONCENTRE SUR L'AUTEUR PRINCIPAL\n",
    "# CETTE PARTIE DE CODE ALIMENTE SEULEUMENT LA PARTIE LIVRE ET SA DESCRIPTION\n",
    "connecteur=sqlite3.connect(BdD)\n",
    "curseur=connecteur.cursor()\n",
    "Heure=heure()\n",
    "print(\"Début du scrapping: \",Heure[1],\" \",Heure[0],\" à \",Heure[2],\":\",Heure[3])\n",
    "for idx,url_BD in enumerate(liensBD[idxsiblocage:len(liensBD)]):\n",
    "    \n",
    "        try:\n",
    "            idx+=1\n",
    "            LinksList=[]\n",
    "            Heure=heure()\n",
    "            addr=\"https://www.babelio.com/\"+url_BD\n",
    "            print(\"Le \",Heure[1],\" \",Heure[0],\" à \",Heure[2],\":\",Heure[3],\"https://www.babelio.com/\"+url_BD,\n",
    "                  \"|| album numéro \",str(idx) , \": \",str(url_BD))\n",
    "            response_BD=requests.post(addr)\n",
    "            soup = BeautifulSoup(response_BD.text)\n",
    "            EAN=soup.find('div',class_=\"livre_refs grey_light\").text\n",
    "            #print(EAN)\n",
    "            Date = EAN[70:102][ EAN[70:102].find( '(' )+1 : EAN[70:102].find( ')' ) ]\n",
    "            EAN=EAN[9:23]\n",
    "            \n",
    "            Title=soup.find(\"h1\", itemprop=\"name\").text\n",
    "            Title=remove_tags(Title)\n",
    "            #print(Title)\n",
    "            Image=soup.find(\"div\",class_=\"col col-4\")\n",
    "            img_src = Image.find('img').attrs['src']\n",
    "            Auteur1=soup.find(\"span\", itemprop=\"name\").text   \n",
    "            #print(Auteur1)\n",
    "            Dessinateur=\"\"\n",
    "            Description=[des for des in (soup.find(\"div\", itemprop=\"description\",class_=\"livre_resume\")) if des!= '[]']\n",
    "            Description=remove_tags(Description)\n",
    "            Genres=[genre.text for genre in (soup.find_all(\"a\",class_=\"tag_t14\"))]\n",
    "            #print(Description)  \n",
    "            Rating=soup.find(\"span\", itemprop=\"ratingValue\").text               \n",
    "            #print(Rating)\n",
    "            time.sleep(8)\n",
    "            #print(\"Sauvegarde en BDD de \",url_BD)\n",
    "            # Eviter les doublons?\n",
    "            curseur.execute(\"Select titre from bd where  titre = ?\",[Title]) #Il faut une liste pour éviter l'erreur de Binding\n",
    "            if len( curseur.fetchall() ) == 0:\n",
    "                curseur.execute(\"INSERT INTO bd (titre,ean,genres,resume,auteur,image,date,rating) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",(Title, EAN, str(Genres), Description, Auteur1, img_src, Date, Rating))\n",
    "                connecteur.commit()\n",
    "        except Exception as e:\n",
    "                if e==TimeoutError:\n",
    "                    print(\"\"\"Erreur de connection\\n il suffit de relancer cette \n",
    "                                        section, le script garde en mémoire l'emplacement \n",
    "                                        lors du problème.\"\"\")\n",
    "                    print(\"Sur le livre \",url_BD)\n",
    "                    print(idx)\n",
    "                    idxsiblocage=idx\n",
    "                    break\n",
    "                else :\n",
    "                    print(\"Erreur...\",e,\"\\n\")\n",
    "                    print(\"Sur le livre \",url_BD)\n",
    "    \n",
    "connecteur.close()\n",
    "print(\"Fin du scrapping: \",Heure[1],\" \",Heure[0],\" à \",Heure[2],\":\",Heure[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c455b53",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c3978",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eeb4f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idxsiblocagecritique=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d3e7f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début du scrapping:  20   10  à  14 : 15\n",
      "Fin du scrapping:  20   10  à  15 : 23\n"
     ]
    }
   ],
   "source": [
    "connecteur=sqlite3.connect(BdD)\n",
    "curseur=connecteur.cursor()\n",
    "Heure=heure()\n",
    "\n",
    "print(\"Début du scrapping: \",Heure[1],\" \",Heure[0],\" à \",Heure[2],\":\",Heure[3])\n",
    "for idx,url_BD in enumerate(liensBD[idxsiblocagecritique:len(liensBD)]):\n",
    "        Heure=heure()\n",
    "        #print(\"Le \",Heure[1],\" \",Here[0],\" à \",Heure[2],\":\",Heure[3],\"|| Scrap des critiques de: \",str(url_BD))\n",
    "    \n",
    "        nbPageCri=1    # Nombres de page de critiques\n",
    "        maxPageCritique=16                  # On ne gère ici que 15 pages, qu'il y en ait plus\n",
    "                                            # moins\n",
    "        while nbPageCri<maxPageCritique:\n",
    "            try:\n",
    "                tmp=\"https://www.babelio.com\"+str(url_BD)+\"/critiques?a=a&pageN=\"+str(nbPageCri)\n",
    "                response_BD=requests.post(tmp)\n",
    "                soup= BeautifulSoup(response_BD.text)\n",
    "                Date=[date.text for date in  soup.find_all(\"span\", style=re.compile('\"Open Sans\",sans-serif' ))]\n",
    "                #print(Date)\n",
    "                Critiques=[remove_tags(critique.text) for critique in  soup.find_all(\"div\", id=re.compile('^cri_'))]\n",
    "                #print(len(Critiques))\n",
    "                Title=soup.find(\"h1\", itemprop=\"name\").text\n",
    "                Title=remove_tags(Title)\n",
    "                #print(Title)\n",
    "                \n",
    "                Lst_Lecteurs=[lecteur.text for lecteur in soup.find_all(\"span\", itemprop=\"name\")]\n",
    "                #print(\"Nombre de lecteurs: \",len(Lst_Lecteurs))\n",
    "                Lst_Notes=soup.find_all(itemprop={\"ratingValue\",\"content\"})\n",
    "                Lst_Notes=([float(s) for s in re.findall(r'-?\\d+\\.?\\d*', str(Lst_Notes))])\n",
    "                \n",
    "                \n",
    "                #print(len(Lst_Notes),len(Lst_Lecteurs))\n",
    "                if len(Lst_Notes)!=len(Lst_Lecteurs):\n",
    "                    del Lst_Notes[0]  # On supprime le 1er élément qui correspond à la note moyenne de la BD\n",
    "                #print(\"Nombre de notes: \",len(Lst_Notes))\n",
    "                \n",
    "             \n",
    "                for num,Note in enumerate(Lst_Notes): # On a une critique par lecteur\n",
    "                    curseur.execute(\"SELECT * from user where first_name=?\",([Lst_Lecteurs[num]]))\n",
    "                    if len( curseur.fetchall() ) == 0:\n",
    "                        curseur.execute(\"INSERT INTO  user (email,password,first_name) VALUES (?,?,?)\",(str(Lst_Lecteurs[num])+\"BD.BD\",\"12345678\",str(Lst_Lecteurs[num])))\n",
    "                        connecteur.commit()\n",
    "                    curseur.execute(\"SELECT titre FROM note where  titre = ? AND auteur= ?\",(Title,Lst_Lecteurs[num])) #Il faut une liste pour éviter l'erreur de Binding\n",
    "                    if len( curseur.fetchall() ) == 0:\n",
    "                        curseur.execute(\"INSERT INTO  note (date,titre,note,data,auteur) VALUES (?,?,?,?,?)\",(Date[num],Title, Note, Critiques[num],Lst_Lecteurs[num]))\n",
    "                        connecteur.commit()\n",
    "            except Exception as e:\n",
    "                if e==TimeoutError:\n",
    "                    print(\"\"\"Erreur de connection\\n il suffit de relancer cette \n",
    "                         section, le script garde en mémoire l'emplacement \n",
    "                         lors du problème.\"\"\")\n",
    "                    idxsiblocagecritique=idx\n",
    "                    break\n",
    "                else :\n",
    "                    print(\"Erreur...\",e,\"\\n\")\n",
    "                    print(\"Sur le livre \",url_BD)                    \n",
    "                \n",
    "            nbPageCri+=1\n",
    "\n",
    "# On ferme les données en base\n",
    "connecteur.close()\n",
    "print(\"Fin du scrapping: \",Heure[1],\" \",Heure[0],\" à \",Heure[2],\":\",Heure[3])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fba0ee9d48c49d9b2f527966fe1318f70f5843a6594ff7e02a61cae89e09a3b4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('sccrapping': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
